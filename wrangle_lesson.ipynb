{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire and Prep - Wrangle\n",
    "\n",
    "In the following lessons, we will walk through the data science pipeline using the following scenario:\n",
    "\n",
    "I'm a university teacher, and I want to know when to worry about a student's progress.  I want to be able to work with any students who are at high risk of failing the class, so that I can try to prevent that from happening.  I have the grades of the three exams and the final grade from last semester's class.  I'm hoping I can build a prediction model that will be able to use these exams to predict the final grade within 5 points average per student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire the Data\n",
    "\n",
    "Let's use pandas to read our csv into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file into pandas DataFrame.\n",
    "\n",
    "df = pd.read_csv(\"student_grades.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample and Summarize\n",
    "\n",
    "Let's take a look at the DataFrame we brought in and document our initial findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>exam1</th>\n",
       "      <th>exam2</th>\n",
       "      <th>exam3</th>\n",
       "      <th>final_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>98.0</td>\n",
       "      <td>93</td>\n",
       "      <td>96</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>83.0</td>\n",
       "      <td>80</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>90</td>\n",
       "      <td>96</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   student_id  exam1  exam2 exam3  final_grade\n",
       "0           1  100.0     90    95           96\n",
       "1           2   98.0     93    96           95\n",
       "2           3   85.0     83    87           87\n",
       "3           4   83.0     80    86           85\n",
       "4           5   93.0     90    96           97"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 104 rows and 5 columns coming in.\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display readable summary statistics for numeric columns. Why isn't exam3 showing up?\n",
    "\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running .info() shows us that the exam3 column is not a numeric data type; it's an object.\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire and Summarize Takeaways\n",
    "\n",
    "- `exam1` has 1 Null value.\n",
    "- `exam1` should likely be of type int64 once the Null value is addressed.\n",
    "- There is likely an odd value in `exam3`, as it should be of type int64 but was read in as an object. We need to find that value.\n",
    "- Given that there are limited attributes and limited observations with missing values, dropping the observations with missing values is probably a good way to go here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Null Values\n",
    "\n",
    "Let's check out some other ways to find Null values when you are dealing with a larger dataframe, especially one with more attributes and more missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.nan values have a float data type. When a column you expect to have an integer data type reads in as a float, this may be signaling that there is one or more Null values present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.isnull().sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total number of Null values in each column of our DataFrame.\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.isnull().any()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any Null values in each column of our DataFrame.\n",
    "\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the names for any columns in our DataFrame with any Null values.\n",
    "\n",
    "df.columns[df.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Odd Values\n",
    "\n",
    "Let's find the odd value in `exam3` that is causing this numeric column to be coerced into an object data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the values and their frequencies from exam3 column.\n",
    "\n",
    "df.exam3.value_counts(dropna=False, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace a whitespace sequence or empty with a NaN value and reassign this manipulation to df.\n",
    "\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that our empty string has been replaced by a null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now .info() shows us that exam3 has a Null value instead of a whitespace disguised as a non-null value.\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Null Values\n",
    "\n",
    "Let's drop observations that have any Null values; in this case, we have so few that we can simply drop rows instead of imputing values to save observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows with any Null values, assign to df, and verify.\n",
    "\n",
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data Types\n",
    "\n",
    "Let's convert any data types we need to at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all column data tyes to int64, reassign to df, and verify.\n",
    "\n",
    "df = df.astype('int')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to fill your missing values with a value instead of dropping the rows. One way to do this is to apply the `.fillna()` method to your dataframe. \n",
    "```python\n",
    "# Default arguments for value and method parameters.\n",
    "\n",
    "df.fillna(value=None, method=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running `.describe()`, we should now see `exam3` listed since we have converted it to a numeric type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Distributions\n",
    "\n",
    "We can plot histograms and/or boxplots to see the distributions of single variables and check for skewness, outliers, and unit scales. *Note, we don't have to split our data before exploring single variables. We DO have to split our data before performing bi- and multi-variate exploration.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sns.displot()`\n",
    "\n",
    "We can use Seaborn's `displot` to display the binned values from a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default is bins=10.\n",
    "\n",
    "sns.displot(x='final_grade', data=df)\n",
    "\n",
    "plt.title('final_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `plt.subplot()` & `.hist()`\n",
    "\n",
    "Here we'll loop through each of the numeric columns of interest and show the distribution of each on a separate subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 3))\n",
    "\n",
    "# List of columns\n",
    "cols = ['exam1', 'exam2', 'exam3', 'final_grade']\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    \n",
    "    # i starts at 0, but plot nos should start at 1\n",
    "    plot_number = i + 1 \n",
    "    \n",
    "    # Create subplot.\n",
    "    plt.subplot(1,4, plot_number)\n",
    "    \n",
    "    # Title with column name.\n",
    "    plt.title(col)\n",
    "    \n",
    "    # Display histogram for column.\n",
    "    df[col].hist(bins=5)\n",
    "    \n",
    "    # Hide gridlines.\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sns.boxplot()`\n",
    "\n",
    "Seaborn's `.boxplot` will default to plotting *all* the numeric variables if we don't specify specific x and y values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't want to plot the `student_id` column.\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# Create boxplots for all but student_id.\n",
    "sns.boxplot(data=df.drop(columns=['student_id']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution Takeaways\n",
    "\n",
    "- All of the column distributions are bimodal. There seem to be more students scoring on the upper and lower edges than in the middle 80s.\n",
    "- `exam3` has the highest median score, exam1 and `final_grade` look to have the same or very similar medians, and `exam2` has the lowest median score.\n",
    "- `exam2` has the least students scoring in the upper half of the grade range and the most scoring in the lower half.\n",
    "- `exam1` and `final_grade` distributions look very similar in these initial charts although `exam1` has a larger range in scores than `final_grade`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Function\n",
    "\n",
    "We finalize these data wrangling steps (acquire and prepare) by writing a function that will reproduce the DataFrame with the necessary changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_grades():\n",
    "    '''\n",
    "    Read student_grades csv file into a pandas DataFrame,\n",
    "    drop student_id column, replace whitespaces with NaN values,\n",
    "    drop any rows with Null values, convert all columns to int64,\n",
    "    return cleaned student grades DataFrame.\n",
    "    '''\n",
    "    # Acquire data from csv file.\n",
    "    grades = pd.read_csv('student_grades.csv')\n",
    "    \n",
    "    # Replace white space values with NaN values.\n",
    "    grades = grades.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    \n",
    "    # Drop all rows with NaN values.\n",
    "    df = grades.dropna()\n",
    "    \n",
    "    # Convert all columns to int64 data types.\n",
    "    df = df.astype('int')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test out or wrangle function from above.\n",
    "\n",
    "df = wrangle_grades()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises I - Required\n",
    "\n",
    "Let's review the steps we take at the beginning of each new module.\n",
    "\n",
    "1. Create a new repository named `regression-exercises` in your GitHub; all of your Regression work will be housed here.\n",
    "1. Clone this repository within your local `codeup-data-science` directory.\n",
    "1. Create a `.gitignore` and make sure your list of 'files to ignore' includes your `env.py` file.\n",
    "1. Ceate a `README.md` file that outlines the contents and purpose of your repository.\n",
    "1. Add, commit, and push these two files.\n",
    "1. Now you can add your `env.py` file to this repository to access the Codeup database server.\n",
    "1. For these exercises, you will create `wrangle.ipynb` and `wrangle.py` files to hold necessary functions.\n",
    "1. As always, add, commit, and push your work often.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises II - Required\n",
    "\n",
    "Let's set up an example scenario as perspective for our regression exercises using the Telco dataset.\n",
    "\n",
    "As a customer analyst for Telco, you want to know who has spent the most money with the company over their lifetime. You have monthly charges and tenure, so you think you will be able to use those two attributes as features to estimate total charges. You need to do this within an average of $5.00 per customer. \n",
    "\n",
    "In these exercises, you will complete the first step toward the above goal: acquire and prepare the necessary Telco data from the telco_churn database in the Codeup database server.\n",
    "\n",
    "1. Acquire `customer_id`, `monthly_charges`, `tenure`, and `total_charges` from the `telco_churn` database for all customers with a 2-year contract.\n",
    "1. Using your acquired Telco data, walk through the summarization and cleaning steps in your `wrangle.ipynb` file like we did above. You may handle the missing values however you feel is appropriate and meaningful; remember to document your process and decisions using markdown and code commenting where helpful.\n",
    "1. End with a `wrangle.py` file that contains the necessary functions to automate your process from acquiring the data to returning a cleaned dataframe with no missing values. Name your final function `wrangle_telco`.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises III - Challenge\n",
    "\n",
    "Let's set up an example scenario as perspective for our regression exercises using the Zillow dataset.\n",
    "\n",
    "As a Codeup data science graduate, you want to show off your skills to the Zillow data science team in hopes of getting an interview for a position you saw pop up on LinkedIn. You thought it might look impressive to build an end-to-end project in which you use some of their Kaggle data to predict property values using some of their available features; who knows, you might even do some feature engineering to blow them away. Your goal is to predict the values of single unit properties using the obervations from 2017.\n",
    "\n",
    "In these exercises, you will complete the first step toward the above goal: acquire and prepare the necessary Zillow data from the zillow database in the Codeup database server.\n",
    "\n",
    "1. Acquire `bedroomcnt`, `bathroomcnt`, `calculatedfinishedsquarefeet`, `taxvaluedollarcnt`, `yearbuilt`, `taxamount`, and `fips` from the `zillow` database for all 'Single Family Residential' properties.\n",
    "1. Using your acquired Zillow data, walk through the summarization and cleaning steps in your `wrangle.ipynb` file like we did above. You may handle the missing values however you feel is appropriate and meaninful; remember to document your process and decisions using markdown and code commenting where helpful.\n",
    "1. Store all of the necessary functions to automate your process from acquiring the data to returning a cleaned dataframe witn no missing values in your `wrangle.py` file. Name your final function `wrangle_zillow`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
